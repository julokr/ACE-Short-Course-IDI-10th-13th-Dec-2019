{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning (Neural Networks)\n",
    "\n",
    "By `Atwine Mugume Twinamatsiko`\n",
    "\n",
    "<img src='deep1.jpeg'/>\n",
    "\n",
    "## What is deep learning?\n",
    "- Deep learning is an AI function that mimics the workings of the human brain in processing data for use in decision making.\n",
    "- Deep learning AI is able to learn from data that is both unstructured and unlabeled\n",
    "\n",
    "\n",
    "## Crash Course Overview\n",
    "We are going to cover a lot of ground in this lesson. Here is an idea of what is ahead:\n",
    "1. Multilayer Perceptrons.\n",
    "2. Neurons, Weights and Activations.\n",
    "3. Networks of Neurons.\n",
    "4. Training Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Neuron\n",
    "A human brain has billions of neurons. Neurons are interconnected nerve cells in the human brain that are involved in processing and transmitting chemical and electrical signals. Dendrites are branches that receive information from other neurons.\n",
    "\n",
    "<img src='deep4.jpg'/>\n",
    "\n",
    "Cell nucleus or Soma processes the information received from dendrites. Axon is a cable that is used by neurons to send information. Synapse is the connection between an axon and other neuron dendrites\n",
    "\n",
    "### Artificial Neuron\n",
    "An artificial neuron is a mathematical function based on a model of biological neurons, where each neuron takes inputs, weighs them separately, sums them up and passes this sum through a nonlinear function to produce output.\n",
    "\n",
    "<img src='deep5.jpeg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurons\n",
    "\n",
    "The building block for neural networks are artificial neurons. These are simple computational units that have weighted input signals and produce an output signal using an activation function.\n",
    "\n",
    "<img src='deep2.png'/>\n",
    "\n",
    "### Neuron Weights\n",
    "\n",
    "You may be familiar with linear regression, in which case the weights on the inputs are very\n",
    "much like the coefficients used in a regression equation. Like linear regression, each neuron also has a bias which can be thought of as an input that always has the value 1.0 and it too must be weighted. For example, a neuron may have two inputs in which case it requires three weights. One for each input and one for the bias\n",
    "\n",
    "### Activation\n",
    "\n",
    "The weighted inputs are summed and passed through an activation function, sometimes called a\n",
    "transfer function. An activation function is a simple mapping of summed weighted input to the\n",
    "output of the neuron. It is called an activation function because it governs the threshold at\n",
    "which the neuron is activated and the strength of the output signal.\n",
    "\n",
    "### Networks of Neurons\n",
    "Neurons are arranged into networks of neurons. A row of neurons is called a layer and one\n",
    "network can have multiple layers. The architecture of the neurons in the network is often called the network topology.\n",
    "\n",
    "<img src='deep3.png'/>\n",
    "\n",
    "\n",
    "### Input or Visible Layers\n",
    "The bottom layer that takes input from your dataset is called the visible layer, because it is\n",
    "the exposed part of the network.\n",
    "\n",
    "\n",
    "### Hidden Layers\n",
    "Layers after the input layer are called hidden layers because they are not directly exposed to\n",
    "the input.\n",
    "\n",
    "\n",
    "### Output Layer\n",
    "\n",
    "The final hidden layer is called the output layer and it is responsible for outputting a value\n",
    "or vector of values that correspond to the format required for the problem. The choice of\n",
    "activation function in the output layer is strongly constrained by the type of problem that you are modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "The classical and still preferred training algorithm for neural networks is called stochastic\n",
    "gradient descent. This is where one row of data is exposed to the network at a time as input.\n",
    "The network processes the input upward activating neurons as it goes to finally produce an\n",
    "output value. This is called a forward pass on the network. It is the type of pass that is also used after the network is trained in order to make predictions on new data.\n",
    "\n",
    "The output of the network is compared to the expected output and an error is calculated.\n",
    "This error is then propagated back through the network, one layer at a time, and the weights\n",
    "are updated according to the amount that they contributed to the error. This clever bit of math is called the Back Propagation algorithm. The process is repeated for all of the examples in your training data. One round of updating the network for the entire training dataset is called an epoch. A network may be trained for tens, hundreds or many thousands of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What the heck if a Perceptron?\n",
    "\n",
    "A Perceptron is an algorithm for supervised learning of binary classifiers. This algorithm enables neurons to learn and processes elements in the training set one at a time.\n",
    "\n",
    "<img src='deep6.jpg'/>\n",
    "\n",
    "There are two types of Perceptrons: Single layer and Multilayer.\n",
    "\n",
    "Single layer Perceptrons can learn only linearly separable patterns.\n",
    "\n",
    "Multilayer Perceptrons or feedforward neural networks with two or more layers have the greater processing power.\n",
    "\n",
    "The Perceptron algorithm learns the weights for the input signals in order to draw a linear decision boundary.\n",
    "\n",
    "\n",
    "### Perceptron Learning Rule\n",
    "Perceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients. The input features are then multiplied with these weights to determine if a neuron fires or not.\n",
    "\n",
    "<img src='deep7.jpg'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Activation Function?\n",
    "\n",
    "It’s just a thing function that you use to get the output of node. It is also known as Transfer Function.\n",
    "\n",
    "It is used to determine the output of neural network like yes or no. It maps the resulting values in between 0 to 1 or -1 to 1 etc. (depending upon the function).\n",
    "\n",
    "### Sigmoid or Logistic Activation Function\n",
    "\n",
    "<img src='deep8.png'/>\n",
    "\n",
    "The main reason why we use sigmoid function is because it exists between (0 to 1). `Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.`\n",
    "The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n",
    "\n",
    "The function is monotonic but function’s derivative is not.\n",
    "\n",
    "The logistic sigmoid function can cause a neural network to get stuck at the training time.\n",
    "\n",
    "The softmax function is a more generalized logistic activation function which is used for multiclass classification.\n",
    "\n",
    "### ReLU (Rectified Linear Unit) Activation Function\n",
    "The ReLU is the most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.\n",
    "\n",
    "<img src='deep9.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between deep learner and machine learning:\n",
    "\n",
    "- The main difference between deep learning and machine learning is due to the way data is presented in the system. Machine learning algorithms almost always require structured data, while deep learning networks rely on layers of ANN (artificial neural networks).\n",
    "- Machine learning algorithms are designed to “learn” to act by understanding labeled data and then use it to produce new results with more datasets. However, when the result is incorrect, there is a need to “teach them”.\n",
    "- Deep learning networks do not require human intervention, as multilevel layers in neural networks place data in a hierarchy of different concepts, which ultimately learn from their own mistakes. However, even they can be wrong if the data quality is not good enough.\n",
    "- Data decides everything. It is the quality of the data that ultimately determines the quality of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron\n",
    "\n",
    "In the Multilayer perceptron, there can more than one linear layer (combinations of neurons). If we take the simple example the three-layer network, first layer will be the input layer and last will be output layer and middle layer will be called hidden layer. We feed our input data into the input layer and take the output from the output layer. We can increase the number of the hidden layer as much as we want, to make the model more complex according to our task.\n",
    "\n",
    "<img src='deep10.jpeg'/>\n",
    "\n",
    "Feed Forward Network, is the most typical neural network model. Its goal is to approximate some function f (). Given, for example, a classifier y = f ∗ (x) that maps an input x to an output class y, the MLP find the best approximation to that classifier by defining a mapping, y = f(x; θ) and learning the best parameters θ for it. The MLP networks are composed of many functions that are chained together. A network with three functions or layers would form f(x) = f (3)(f (2)(f (1)(x))). Each of these layers is composed of units that perform an affine transformation of a linear sum of inputs. Each layer is represented as y = f(WxT + b). Where f is the activation function (covered below), W is the set of parameter, or weights, in the layer, x is the input vector, which can also be the output of the previous layer, and b is the bias vector. The layers of an MLP consists of several fully connected layers because each unit in a layer is connected to all the units in the previous layer. In a fully connected layer, the parameters of each unit are independent of the rest of the units in the layer, that means each unit possess a unique set of weights.\n",
    "\n",
    "## Forward pass\n",
    "In this step of training the model, we just pass the input to model and multiply with weights and add bias at every layer and find the calculated output of the model.\n",
    "\n",
    "<img src='deep11.png'/>\n",
    "\n",
    "## Loss Calculate\n",
    "When we pass the data instance(or one example) we will get some output from the model that is called Predicted output(pred_out) and we have the label with the data that is real output or expected output(Expect_out). Based upon these both we calculate the loss that we have to backpropagate(using Backpropagation algorithm). There is various Loss Function that we use based on our output and requirement.\n",
    "\n",
    "## Backward Pass\n",
    "After calculating the loss, we backpropagate the loss and updates the weights of the model by using gradient. This is the main step in the training of the model. In this step, weights will adjust according to the gradient flow in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "### What is NLP (Natural Language Processing)?\n",
    "NLP is a subfield of computer science and artificial intelligence concerned with interactions between computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech.\n",
    "\n",
    "For example, we can use NLP to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on.\n",
    "\n",
    "\n",
    "### Introduction to the NLTK library for Python\n",
    "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project.\n",
    "\n",
    "[Link](https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63)\n",
    "\n",
    "## The Basics of NLP for Text\n",
    "In this article, we’ll cover the following topics:\n",
    "- Sentence Tokenization\n",
    "- Word Tokenization\n",
    "- Text Lemmatization and Stemming\n",
    "- Stop Words\n",
    "- Regex\n",
    "- Bag-of-Words\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization\n",
    "Sentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.\n",
    "\n",
    "#### Example:\n",
    "Let’s look a piece of text about a famous board game called backgammon.\n",
    "\n",
    ">>Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
    "\n",
    "To apply a sentence tokenization with NLTK we can use the `nltk.sent_tokenize` function.\n",
    "\n",
    "```python\n",
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()\n",
    "\n",
    "```\n",
    "\n",
    "#### As an output, we get the 3 component sentences separately.\n",
    "\n",
    "```python\n",
    "Backgammon is one of the oldest known board games.\n",
    "\n",
    "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
    "\n",
    "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider.\n",
    "\n",
    "### Example:\n",
    "Let’s use the sentences from the previous step and see how we can apply word tokenization on them. We can use the `nltk.word_tokenize` function.\n",
    "\n",
    "```python\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    print()\n",
    "    \n",
    "    ```\n",
    "    \n",
    " ### Output:   \n",
    " \n",
    " ```python\n",
    "['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']\n",
    "\n",
    "['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']\n",
    "\n",
    "['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link1](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)\n",
    "\n",
    "[Link1](https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63)\n",
    "\n",
    "[Link](https://medium.com/analytics-vidhya/introduction-to-natural-language-processing-part-1-777f972cc7b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
